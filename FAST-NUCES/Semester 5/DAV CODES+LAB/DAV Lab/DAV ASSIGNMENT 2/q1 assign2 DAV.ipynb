{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ea29fd-54bf-44d7-b71a-3307e32984b5",
   "metadata": {},
   "source": [
    "<center><strong><h1>Assignment#2</center></strong></h1>\n",
    "\n",
    "Name: Muhammad Ahmed Javed\n",
    "\n",
    "Roll Number: 21-7692\n",
    "\n",
    "---\n",
    "<center><strong><h3>Question 1: Image Processing</center></strong></h3>\n",
    "\n",
    "You are provided with the image dataset of cats and dogs. You have to write Python code and\n",
    "implement the following tasks:\n",
    "\n",
    "● Convert RGB images to grayscale using following formula:\n",
    "\n",
    "<center>grayscale image = ((0.3 * R) + (0.59 * G) + (0.11 * B))</center> \n",
    "\n",
    "● Design a 3x3 convolution and correlation filter for this dataset.\n",
    "\n",
    "● Design 1st order derivative and 2nd Order derivative filter and apply on input images.\n",
    "\n",
    "● Identify the type of noise (Gaussian or Salt and Pepper) in images.\n",
    "\n",
    "● Based on your assumption apply relative filter on input images to remove the noise.\n",
    "\n",
    "● Implement Laplacian edge detection on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ba59e67-1ef8-4ca2-9b27-7b11b338f30f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\bits\\anaconda3\\lib\\site-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\bits\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\BiTS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\BiTS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "!pip install opencv-python\n",
    "import csv,cv2,os,re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "736ff305-709f-4115-97ab-1b12ecbbbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_dataset=\"Image Dataset\"\n",
    "output_folders={\"Grayscale\":\"Grayscale\",\"Convolution\":\"Convolution\",\"Correlation\":\"Correlation\",\n",
    "\"Derivative_One\":\"Derivative_One\",\"Derivative_Two\":\"Derivative_Two\",\"NoiseRemoval\":\"NoiseRemoval\",\n",
    "\"EdgeDetection\":\"EdgeDetection\",\"HistroGrams\":\"HistoGrams\"}\n",
    "for task, output_folder in output_folders.items():\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "image_files = [f for f in os.listdir(Image_dataset) if f.endswith(('.jpg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b93c57b8-b7fd-46cc-9993-b1e903e99a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_kernel=np.array([[-1, -1, -1],[-1, 8, -1],[-1, -1, -1]])\n",
    "correlation_kernal=np.array([[0, -1, 0],[-1, 5, -1],[0, -1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14815da0-5614-415a-ad41-a8f1cc9fbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for file in image_files:\n",
    "    image_path = os.path.join(Image_dataset, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_scaled_image=np.dot(image[...,:3],[0.3,0.59,0.11])\n",
    "    new_gray_scled_image_name=f\"{i}.jpg\"\n",
    "    cv2.imwrite(os.path.join(output_folders[\"Grayscale\"],new_gray_scled_image_name),gray_scaled_image)\n",
    "    convolution_image=cv2.filter2D(gray_scaled_image,-1,convolution_kernel)\n",
    "    gray_scaled_image_format=gray_scaled_image.astype(np.uint8)\n",
    "    cv2.imwrite(os.path.join(output_folders[\"Convolution\"],new_gray_scled_image_name),convolution_image) \n",
    "    correlation_image=cv2.filter2D(gray_scaled_image,-1,correlation_kernal)\n",
    "    cv2.imwrite(os.path.join(output_folders[\"Correlation\"],new_gray_scled_image_name),correlation_image)\n",
    "    sobel_x_direction_image=cv2.Sobel(image,cv2.CV_64F,1,0,ksize=3)\n",
    "    cv2.imwrite(os.path.join(output_folders[\"Derivative_One\"],new_gray_scled_image_name),sobel_x_direction_image)\n",
    "    sobel_y_direction_image=cv2.Sobel(image,cv2.CV_64F,0,1,ksize=3)\n",
    "    cv2.imwrite(os.path.join(output_folders[\"Derivative_Two\"],new_gray_scled_image_name),sobel_y_direction_image)\n",
    "    hist=cv2.calcHist([gray_scaled_image_format],[0],None,[256],[0, 256])\n",
    "    plt.title(f'{j}.jpg Histogram')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.plot(hist)\n",
    "    plt.savefig(os.path.join('HistoGrams', f'{i}.png'))\n",
    "    plt.close() \n",
    "    filtered_image = cv2.GaussianBlur(gray_scaled_image,(5,5),0)\n",
    "    cv2.imwrite(os.path.join(output_folders[\"NoiseRemoval\"],new_gray_scled_image_name),filtered_image)\n",
    "    laplacian_applied_image = cv2.Laplacian(gray_scaled_image, cv2.CV_64F)\n",
    "    cv2.imwrite(os.path.join(output_folders[\"EdgeDetection\"],new_gray_scled_image_name),laplacian_applied_image)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5f192-91a8-40f9-8dba-48b25c8ec5f4",
   "metadata": {},
   "source": [
    "Open HistoGrams Folder and observe that the Histograms of Gray Scaled Images follow Normal Distribution.Therefore,There ia a noise of type (Gaussian)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f871c4f-c235-44bb-be45-776bcba9814d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><strong><h3>Question 2: Text Processing</center></strong></h3>\n",
    "\n",
    "You are provided with a labelled dataset that contains data of tweets with their corresponding\n",
    "labels (0 for hate speech and 1 for neutral text). Use the given dataset to perform the following\n",
    "tasks:\n",
    "\n",
    "● Load the dataset using pandas and split it into training and testing sets.\n",
    "\n",
    "● Implement the Bag of Words Feature Extraction Technique.\n",
    "\n",
    "● Implement the TF-IDF Feature Extraction Technique.\n",
    "\n",
    "● Preprocess the provided data with inbuilt libraries using:\n",
    "\n",
    "➢ Tokenization: Tokenize the text into individual words or tokens.\n",
    "\n",
    "➢ Lowercasing: Convert all tokens to lowercase.\n",
    "\n",
    "➢ Stop word Removal: Remove common English stop words, i.e., a, is, am, the, as, of,\n",
    "\n",
    "to, from.\n",
    "\n",
    "➢ Smoothing: Implement Laplace smoothing to handle out-of-vocabulary words.\n",
    "\n",
    "➢ Normalization: Normalize the text by removing punctuation and special characters.\n",
    "\n",
    "➢ Stemming: Apply stemming to reduce words to their base form.\n",
    "\n",
    "➢ Lemmatization: Implement lemmatization to reduce words to their dictionary form.\n",
    "\n",
    "● Build a logistic regression model from scratch (do not use inbuilt function) for the text\n",
    "\n",
    "classification. Train the logistic regression model using both the BOW and TF-IDF\n",
    "representations. (Use sigmoid activation function).\n",
    "\n",
    "● Evaluate the performance of the logistic regression model using the accuracy, precision,\n",
    "recall, and F1-score for both classes.\n",
    "\n",
    "● Compare the performance of the logistic regression model trained on BOW and TF-IDF\n",
    "representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52a76dc0-a213-4025-920d-1f1974615557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                             Tweets\n",
       "0      1  !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1      0  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2      0  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=pd.read_csv(\"Labelled  Tweets.csv\")\n",
    "file.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50ec5e45-0e78-4da7-bd17-e1a95ab25fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [!, !, !, RT, @, mayasolovely, :, As, a, woman...\n",
       "1    [!, !, !, !, !, RT, @, mleew17, :, boy, dats, ...\n",
       "2    [!, !, !, !, !, !, !, RT, @, UrKindOfBrand, Da...\n",
       "3    [!, !, !, !, !, !, !, !, !, RT, @, C_G_Anderso...\n",
       "4    [!, !, !, !, !, !, !, !, !, !, !, !, !, RT, @,...\n",
       "Name: Tokenized_Tweets, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['Tokenized_Tweets']=file.Tweets.apply(nltk.word_tokenize)\n",
    "file['Tokenized_Tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f44a523f-186d-44b7-a1e2-f5b63e377eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [!, !, !, rt, @, mayasolovely, :, as, a, woman...\n",
       "1    [!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...\n",
       "2    [!, !, !, !, !, !, !, rt, @, urkindofbrand, da...\n",
       "3    [!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...\n",
       "4    [!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...\n",
       "Name: Lowered_the_Tokenized_Tweets, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['Lowered_the_Tokenized_Tweets'] = file['Tokenized_Tweets'].apply(lambda tokens: [token.lower() for token in tokens])\n",
    "file.Lowered_the_Tokenized_Tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "226ad205-a6fb-4f16-bf7d-ee9203fd6db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tokenized_Tweets</th>\n",
       "      <th>Lowered_the_Tokenized_Tweets</th>\n",
       "      <th>Removed_STop_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[!, !, !, RT, @, mayasolovely, :, As, a, woman...</td>\n",
       "      <td>[!, !, !, rt, @, mayasolovely, :, as, a, woman...</td>\n",
       "      <td>[!, !, !, rt, @, mayasolovely, :, woman, n't, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[!, !, !, !, !, RT, @, mleew17, :, boy, dats, ...</td>\n",
       "      <td>[!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...</td>\n",
       "      <td>[!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[!, !, !, !, !, !, !, RT, @, UrKindOfBrand, Da...</td>\n",
       "      <td>[!, !, !, !, !, !, !, rt, @, urkindofbrand, da...</td>\n",
       "      <td>[!, !, !, !, !, !, !, rt, @, urkindofbrand, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, RT, @, C_G_Anderso...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, !, !, !, !, RT, @,...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                             Tweets  \\\n",
       "0      1  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1      0  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2      0  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3      0  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4      0  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                    Tokenized_Tweets  \\\n",
       "0  [!, !, !, RT, @, mayasolovely, :, As, a, woman...   \n",
       "1  [!, !, !, !, !, RT, @, mleew17, :, boy, dats, ...   \n",
       "2  [!, !, !, !, !, !, !, RT, @, UrKindOfBrand, Da...   \n",
       "3  [!, !, !, !, !, !, !, !, !, RT, @, C_G_Anderso...   \n",
       "4  [!, !, !, !, !, !, !, !, !, !, !, !, !, RT, @,...   \n",
       "\n",
       "                        Lowered_the_Tokenized_Tweets  \\\n",
       "0  [!, !, !, rt, @, mayasolovely, :, as, a, woman...   \n",
       "1  [!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...   \n",
       "2  [!, !, !, !, !, !, !, rt, @, urkindofbrand, da...   \n",
       "3  [!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...   \n",
       "4  [!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...   \n",
       "\n",
       "                                  Removed_STop_WORDS  \n",
       "0  [!, !, !, rt, @, mayasolovely, :, woman, n't, ...  \n",
       "1  [!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...  \n",
       "2  [!, !, !, !, !, !, !, rt, @, urkindofbrand, da...  \n",
       "3  [!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...  \n",
       "4  [!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "file['Removed_STop_WORDS']=file['Lowered_the_Tokenized_Tweets'].apply(lambda tokens:[token for token in tokens if token not in stop_words])\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7b533d4-2851-44af-b5a3-d595bd44f24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tokenized_Tweets</th>\n",
       "      <th>Lowered_the_Tokenized_Tweets</th>\n",
       "      <th>Removed_STop_WORDS</th>\n",
       "      <th>Normalized_Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>[!, !, !, RT, @, mayasolovely, :, As, a, woman...</td>\n",
       "      <td>[!, !, !, rt, @, mayasolovely, :, as, a, woman...</td>\n",
       "      <td>[!, !, !, rt, @, mayasolovely, :, woman, n't, ...</td>\n",
       "      <td>[, , , rt, , mayasolovely, , woman, nt, compla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>[!, !, !, !, !, RT, @, mleew17, :, boy, dats, ...</td>\n",
       "      <td>[!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...</td>\n",
       "      <td>[!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...</td>\n",
       "      <td>[, , , , , rt, , mleew17, , boy, dats, cold, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>[!, !, !, !, !, !, !, RT, @, UrKindOfBrand, Da...</td>\n",
       "      <td>[!, !, !, !, !, !, !, rt, @, urkindofbrand, da...</td>\n",
       "      <td>[!, !, !, !, !, !, !, rt, @, urkindofbrand, da...</td>\n",
       "      <td>[, , , , , , , rt, , urkindofbrand, dawg, , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, RT, @, C_G_Anderso...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...</td>\n",
       "      <td>[, , , , , , , , , rt, , cganderson, , , vivab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, !, !, !, !, RT, @,...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...</td>\n",
       "      <td>[!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...</td>\n",
       "      <td>[, , , , , , , , , , , , , rt, , shenikarobert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                             Tweets  \\\n",
       "0      1  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1      0  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2      0  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3      0  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4      0  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                    Tokenized_Tweets  \\\n",
       "0  [!, !, !, RT, @, mayasolovely, :, As, a, woman...   \n",
       "1  [!, !, !, !, !, RT, @, mleew17, :, boy, dats, ...   \n",
       "2  [!, !, !, !, !, !, !, RT, @, UrKindOfBrand, Da...   \n",
       "3  [!, !, !, !, !, !, !, !, !, RT, @, C_G_Anderso...   \n",
       "4  [!, !, !, !, !, !, !, !, !, !, !, !, !, RT, @,...   \n",
       "\n",
       "                        Lowered_the_Tokenized_Tweets  \\\n",
       "0  [!, !, !, rt, @, mayasolovely, :, as, a, woman...   \n",
       "1  [!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...   \n",
       "2  [!, !, !, !, !, !, !, rt, @, urkindofbrand, da...   \n",
       "3  [!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...   \n",
       "4  [!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...   \n",
       "\n",
       "                                  Removed_STop_WORDS  \\\n",
       "0  [!, !, !, rt, @, mayasolovely, :, woman, n't, ...   \n",
       "1  [!, !, !, !, !, rt, @, mleew17, :, boy, dats, ...   \n",
       "2  [!, !, !, !, !, !, !, rt, @, urkindofbrand, da...   \n",
       "3  [!, !, !, !, !, !, !, !, !, rt, @, c_g_anderso...   \n",
       "4  [!, !, !, !, !, !, !, !, !, !, !, !, !, rt, @,...   \n",
       "\n",
       "                                    Normalized_Tweet  \n",
       "0  [, , , rt, , mayasolovely, , woman, nt, compla...  \n",
       "1  [, , , , , rt, , mleew17, , boy, dats, cold, ,...  \n",
       "2  [, , , , , , , rt, , urkindofbrand, dawg, , , ...  \n",
       "3  [, , , , , , , , , rt, , cganderson, , , vivab...  \n",
       "4  [, , , , , , , , , , , , , rt, , shenikarobert...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['Normalized_Tweet']=file['Removed_STop_WORDS'].apply(lambda tokens: [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens])\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1af49d4f-c083-420a-adb7-f1d2a8e6af16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [, , , rt, , mayasolov, , woman, nt, complain,...\n",
       "1    [, , , , , rt, , mleew17, , boy, dat, cold, , ...\n",
       "2    [, , , , , , , rt, , urkindofbrand, dawg, , , ...\n",
       "3    [, , , , , , , , , rt, , cganderson, , , vivab...\n",
       "4    [, , , , , , , , , , , , , rt, , shenikarobert...\n",
       "Name: STEMMERED_TWEET, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "file['STEMMERED_TWEET']=file['Normalized_Tweet'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "file.STEMMERED_TWEET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e7fbad3-86d1-458e-9dfc-2ee1397166ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [, , , rt, , mayasolov, , woman, nt, complain,...\n",
       "1    [, , , , , rt, , mleew17, , boy, dat, cold, , ...\n",
       "2    [, , , , , , , rt, , urkindofbrand, dawg, , , ...\n",
       "3    [, , , , , , , , , rt, , cganderson, , , vivab...\n",
       "4    [, , , , , , , , , , , , , rt, , shenikarobert...\n",
       "Name: LEMMATIZED_TWEETS, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "file['LEMMATIZED_TWEETS'] = file['STEMMERED_TWEET'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "file.LEMMATIZED_TWEETS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70ebfa0b-0325-444c-8cc5-2ef52d418286",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.to_csv(\"Preprocess_Labelled_Tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec277dad-a569-4d6e-8660-0c337673fcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tokenized_Tweets</th>\n",
       "      <th>Lowered_the_Tokenized_Tweets</th>\n",
       "      <th>Removed_STop_WORDS</th>\n",
       "      <th>Normalized_Tweet</th>\n",
       "      <th>STEMMERED_TWEET</th>\n",
       "      <th>LEMMATIZED_TWEETS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['!', '!', '!', 'RT', '@', 'mayasolovely', ':'...</td>\n",
       "      <td>['!', '!', '!', 'rt', '@', 'mayasolovely', ':'...</td>\n",
       "      <td>['!', '!', '!', 'rt', '@', 'mayasolovely', ':'...</td>\n",
       "      <td>['', '', '', 'rt', '', 'mayasolovely', '', 'wo...</td>\n",
       "      <td>['', '', '', 'rt', '', 'mayasolov', '', 'woman...</td>\n",
       "      <td>['', '', '', 'rt', '', 'mayasolov', '', 'woman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>['!', '!', '!', '!', '!', 'RT', '@', 'mleew17'...</td>\n",
       "      <td>['!', '!', '!', '!', '!', 'rt', '@', 'mleew17'...</td>\n",
       "      <td>['!', '!', '!', '!', '!', 'rt', '@', 'mleew17'...</td>\n",
       "      <td>['', '', '', '', '', 'rt', '', 'mleew17', '', ...</td>\n",
       "      <td>['', '', '', '', '', 'rt', '', 'mleew17', '', ...</td>\n",
       "      <td>['', '', '', '', '', 'rt', '', 'mleew17', '', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', 'RT', '@',...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', 'rt', '@',...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', 'rt', '@',...</td>\n",
       "      <td>['', '', '', '', '', '', '', 'rt', '', 'urkind...</td>\n",
       "      <td>['', '', '', '', '', '', '', 'rt', '', 'urkind...</td>\n",
       "      <td>['', '', '', '', '', '', '', 'rt', '', 'urkind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', '!', '!', ...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', '!', '!', ...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', '!', '!', ...</td>\n",
       "      <td>['', '', '', '', '', '', '', '', '', 'rt', '',...</td>\n",
       "      <td>['', '', '', '', '', '', '', '', '', 'rt', '',...</td>\n",
       "      <td>['', '', '', '', '', '', '', '', '', 'rt', '',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', '!', '!', ...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', '!', '!', ...</td>\n",
       "      <td>['!', '!', '!', '!', '!', '!', '!', '!', '!', ...</td>\n",
       "      <td>['', '', '', '', '', '', '', '', '', '', '', '...</td>\n",
       "      <td>['', '', '', '', '', '', '', '', '', '', '', '...</td>\n",
       "      <td>['', '', '', '', '', '', '', '', '', '', '', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                             Tweets  \\\n",
       "0      1  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1      0  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2      0  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3      0  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4      0  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                    Tokenized_Tweets  \\\n",
       "0  ['!', '!', '!', 'RT', '@', 'mayasolovely', ':'...   \n",
       "1  ['!', '!', '!', '!', '!', 'RT', '@', 'mleew17'...   \n",
       "2  ['!', '!', '!', '!', '!', '!', '!', 'RT', '@',...   \n",
       "3  ['!', '!', '!', '!', '!', '!', '!', '!', '!', ...   \n",
       "4  ['!', '!', '!', '!', '!', '!', '!', '!', '!', ...   \n",
       "\n",
       "                        Lowered_the_Tokenized_Tweets  \\\n",
       "0  ['!', '!', '!', 'rt', '@', 'mayasolovely', ':'...   \n",
       "1  ['!', '!', '!', '!', '!', 'rt', '@', 'mleew17'...   \n",
       "2  ['!', '!', '!', '!', '!', '!', '!', 'rt', '@',...   \n",
       "3  ['!', '!', '!', '!', '!', '!', '!', '!', '!', ...   \n",
       "4  ['!', '!', '!', '!', '!', '!', '!', '!', '!', ...   \n",
       "\n",
       "                                  Removed_STop_WORDS  \\\n",
       "0  ['!', '!', '!', 'rt', '@', 'mayasolovely', ':'...   \n",
       "1  ['!', '!', '!', '!', '!', 'rt', '@', 'mleew17'...   \n",
       "2  ['!', '!', '!', '!', '!', '!', '!', 'rt', '@',...   \n",
       "3  ['!', '!', '!', '!', '!', '!', '!', '!', '!', ...   \n",
       "4  ['!', '!', '!', '!', '!', '!', '!', '!', '!', ...   \n",
       "\n",
       "                                    Normalized_Tweet  \\\n",
       "0  ['', '', '', 'rt', '', 'mayasolovely', '', 'wo...   \n",
       "1  ['', '', '', '', '', 'rt', '', 'mleew17', '', ...   \n",
       "2  ['', '', '', '', '', '', '', 'rt', '', 'urkind...   \n",
       "3  ['', '', '', '', '', '', '', '', '', 'rt', '',...   \n",
       "4  ['', '', '', '', '', '', '', '', '', '', '', '...   \n",
       "\n",
       "                                     STEMMERED_TWEET  \\\n",
       "0  ['', '', '', 'rt', '', 'mayasolov', '', 'woman...   \n",
       "1  ['', '', '', '', '', 'rt', '', 'mleew17', '', ...   \n",
       "2  ['', '', '', '', '', '', '', 'rt', '', 'urkind...   \n",
       "3  ['', '', '', '', '', '', '', '', '', 'rt', '',...   \n",
       "4  ['', '', '', '', '', '', '', '', '', '', '', '...   \n",
       "\n",
       "                                   LEMMATIZED_TWEETS  \n",
       "0  ['', '', '', 'rt', '', 'mayasolov', '', 'woman...  \n",
       "1  ['', '', '', '', '', 'rt', '', 'mleew17', '', ...  \n",
       "2  ['', '', '', '', '', '', '', 'rt', '', 'urkind...  \n",
       "3  ['', '', '', '', '', '', '', '', '', 'rt', '',...  \n",
       "4  ['', '', '', '', '', '', '', '', '', '', '', '...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Preprocess_Labelled_Tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e313ab2-10b7-4c65-8f83-5282b21d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['LEMMATIZED_TWEETS']\n",
    "y=df['Class'] \n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ede50270-5153-4514-875b-eeff236a407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ac89ee9f-721f-4c7a-848e-1e9161f5f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

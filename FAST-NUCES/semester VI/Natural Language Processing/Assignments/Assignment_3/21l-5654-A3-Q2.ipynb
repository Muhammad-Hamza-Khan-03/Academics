{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I saved pre processed text in csv file m using that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ایٹم بم بنایا ھے بھائی ایٹم بمب کوٹ لکھپت اتفا...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چندے انقلاب عمران خان وزیر اعظم بن سکتے</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ٹویٹر خیال</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سرچ انجن گوگل نائب صدر فضا فٹ بلندی چھلانگ عال...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اسکی لہریں یار أ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Class\n",
       "0  ایٹم بم بنایا ھے بھائی ایٹم بمب کوٹ لکھپت اتفا...      2\n",
       "1            چندے انقلاب عمران خان وزیر اعظم بن سکتے      0\n",
       "2                                         ٹویٹر خیال      1\n",
       "3  سرچ انجن گوگل نائب صدر فضا فٹ بلندی چھلانگ عال...      2\n",
       "4                                   اسکی لہریں یار أ      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urdu_dataset = pd.read_csv(\"Analysed_Urdu_Tweets.csv\",index_col=False)\n",
    "urdu_dataset.drop(urdu_dataset.columns[0],axis=1,inplace=True)\n",
    "urdu_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (749, 31)\n",
      "Testing set shape: (250, 31)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## WORDS in dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(urdu_dataset['Tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(urdu_dataset['Tweet'])\n",
    "\n",
    "max_sequence_length = max(len(x) for x in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "## Split the dataset\n",
    "# Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, urdu_dataset['Class'], test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\",X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in d:\\python64\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in d:\\python64\\lib\\site-packages (from tensorflow_hub) (1.21.6)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in d:\\python64\\lib\\site-packages (from tensorflow_hub) (3.19.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import tensorflow_hub as hub\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "glove_vectors = api.load(\"glove-twitter-200\")\n",
    "# glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMETERS DECLARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(seq) for seq in X_train])\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_weights = create_embedding_matrix(word2vec_model, tokenizer)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_matrix = np.zeros((vocab_size,word2vec_model.vector_size))\n",
    "model_eval=dict()\n",
    "\n",
    "\n",
    "num_layers =3\n",
    "dropout_rate=0.7\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "for w,i in tokenizer.word_index.items():\n",
    "    if w in word2vec_model:\n",
    "        embed_matrix[i] = word2vec_model[w]\n",
    "word2vec_weights = embed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in d:\\python64\\lib\\site-packages (2.11.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "18/18 [==============================] - 22s 557ms/step - loss: 0.0000e+00 - accuracy: 0.4688 - val_loss: 0.0000e+00 - val_accuracy: 0.5160\n",
      "Epoch 2/8\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.0000e+00 - accuracy: 0.4100 - val_loss: 0.0000e+00 - val_accuracy: 0.5160\n",
      "Epoch 3/8\n",
      "18/18 [==============================] - 2s 138ms/step - loss: 0.0000e+00 - accuracy: 0.4688 - val_loss: 0.0000e+00 - val_accuracy: 0.5160\n",
      "Epoch 4/8\n",
      "18/18 [==============================] - 4s 215ms/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.3191\n",
      "Epoch 5/8\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.0000e+00 - accuracy: 0.4902 - val_loss: 0.0000e+00 - val_accuracy: 0.2287\n",
      "Epoch 6/8\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.0000e+00 - accuracy: 0.4884 - val_loss: 0.0000e+00 - val_accuracy: 0.2181\n",
      "Epoch 7/8\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.0000e+00 - accuracy: 0.4920 - val_loss: 0.0000e+00 - val_accuracy: 0.2234\n",
      "Epoch 8/8\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.0000e+00 - accuracy: 0.5009 - val_loss: 0.0000e+00 - val_accuracy: 0.1702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251c669a608>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM -> 3,0.7 ->0.66\n",
    "# nested for for dropout and num_layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=8, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 18ms/step\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.2160\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LSTM(no embedding)': {'Precision': 0.23621514773200167,\n",
       "  'Recall': 0.4116575591985428,\n",
       "  'F1 score': 0.18650082175407137,\n",
       "  'accuracy': 0.2160000056028366}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval['LSTM(no embedding)'] = {'Precision':precision,'Recall':recall,'F1 score':f1,'accuracy':accuracy}\n",
    "model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 12s 371ms/step - loss: 0.0000e+00 - accuracy: 0.0735 - val_loss: 0.0000e+00 - val_accuracy: 0.0067\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 2s 182ms/step - loss: 0.0000e+00 - accuracy: 0.3022 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0000e+00 - accuracy: 0.4958 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 2s 187ms/step - loss: 0.0000e+00 - accuracy: 0.4925 - val_loss: 0.0000e+00 - val_accuracy: 0.3133\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 2s 191ms/step - loss: 0.0000e+00 - accuracy: 0.5025 - val_loss: 0.0000e+00 - val_accuracy: 0.3600\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0000e+00 - accuracy: 0.5075 - val_loss: 0.0000e+00 - val_accuracy: 0.3067\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.0000e+00 - accuracy: 0.5109 - val_loss: 0.0000e+00 - val_accuracy: 0.2600\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 2s 158ms/step - loss: 0.0000e+00 - accuracy: 0.5125 - val_loss: 0.0000e+00 - val_accuracy: 0.2533\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.0000e+00 - accuracy: 0.5125 - val_loss: 0.0000e+00 - val_accuracy: 0.2600\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 2s 173ms/step - loss: 0.0000e+00 - accuracy: 0.5109 - val_loss: 0.0000e+00 - val_accuracy: 0.2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251c6695cc8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_weights), output_dim=300,weights=[word2vec_weights], input_length=max_length))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 22ms/step\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.2920\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "precision_word2vec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_word2vec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1_word2vec = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "loss, accuracy_word2vec = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval['LSTM(embedding: word2vec)'] = {'Precision':precision_word2vec,'Recall':recall_word2vec,'F1 score':f1_word2vec,'accuracy':accuracy_word2vec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (200,) into shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38892\\1692191605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglove_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (200,) into shape (100,)"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_vectors.key_to_index:\n",
    "        embedding_matrix[i] = glove_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9498 - accuracy: 0.4700"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38892\\4219111409.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test,y_test)) i think\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=3, activation='softmax')) #3 classes\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=0.25)\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test,y_test)) i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7088 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 27ms/step - loss: 0.7918 - accuracy: 0.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# loss, accuracy_glove = model.evaluate(X_test, y_pred)\n",
    "# precision_glove = precision_score(y_pred, y_pred_labels, average='macro')\n",
    "# recall_glove = recall_score(y_pred, y_pred_labels, average='macro')\n",
    "# f1_glove = f1_score(y_pred, y_pred_labels, average='macro')\n",
    "precision_glove = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_glove = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1_glove = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "loss, accuracy_glove = model.evaluate(X_test, y_test)\n",
    "model_eval['LSTM(embedding: GLOVE)'] = {'Precision':precision_glove,'Recall':recall_glove,'F1 score':f1_glove,'accuracy':accuracy_glove}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((vocab_size,fasttext_model.vector_size))\n",
    "\n",
    "for w,i in tokenizer.word_index.items():\n",
    "    if w in fasttext_model and w in word2vec_model:\n",
    "        embed_matrix[i] = word2vec_model[w]\n",
    "fasttext_weights = embed_matrix    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer embedding_7 weight shape (4442, 300) is not compatible with provided weight shape (4442, 100).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38892\\16961147.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mref_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                     raise ValueError(\n\u001b[1;32m-> 1810\u001b[1;33m                         \u001b[1;34mf\"Layer {self.name} weight shape {ref_shape} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m                         \u001b[1;34m\"is not compatible with provided weight \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m                         \u001b[1;34mf\"shape {weight_shape}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer embedding_7 weight shape (4442, 300) is not compatible with provided weight shape (4442, 100)."
     ]
    }
   ],
   "source": [
    "input_dim = len(fasttext_weights)\n",
    "\n",
    "input_dim = vocab_size\n",
    "input_length = max_length\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=300, weights=[embedding_matrix], input_length=input_length, trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "precision_fasttext = precision_score(y_test, y_pred, average='macro')\n",
    "recall_fasttext = recall_score(y_test, y_pred, average='macro')\n",
    "f1_fasttext = f1_score(y_test, y_pred, average='macro')\n",
    "loss, accuracy_fasttext = model.evaluate(X_test, y_test)\n",
    "model_eval['LSTM(embedding: fast-text)'] = {'Precision':precision_fasttext,'Recall':recall_fasttext,'F1 score':f1_fasttext,'accuracy':accuracy_fasttext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Precision    Recall  F1 score  accuracy\n",
      "LSTM(no embedding)         0.162667  0.333333  0.218638       NaN\n",
      "GRU(embedding: word2vec)   0.162667  0.333333  0.218638       NaN\n",
      "LSTM(embedding: GLOVE)     0.162667  0.333333  0.218638     0.488\n"
     ]
    }
   ],
   "source": [
    "table = pd.DataFrame.from_dict(model_eval, orient='index')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

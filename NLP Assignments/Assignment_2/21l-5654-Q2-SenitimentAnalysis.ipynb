{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623495523</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon Dec 01 20:46:01 +0000 2014</td>\n",
       "      <td>WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>623495527</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon Dec 01 21:09:50 +0000 2014</td>\n",
       "      <td>@apple Contact sync between Yosemite and iOS8 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>623495529</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon Dec 01 21:35:14 +0000 2014</td>\n",
       "      <td>WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623495536</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon Dec 01 23:55:55 +0000 2014</td>\n",
       "      <td>@Apple, For the love of GAWD, CENTER the '1'on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>623495537</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue Dec 02 00:06:05 +0000 2014</td>\n",
       "      <td>i get the storage almost full notification lit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id sentiment                            date  \\\n",
       "0  623495523         1  Mon Dec 01 20:46:01 +0000 2014   \n",
       "1  623495527         1  Mon Dec 01 21:09:50 +0000 2014   \n",
       "2  623495529         1  Mon Dec 01 21:35:14 +0000 2014   \n",
       "3  623495536         1  Mon Dec 01 23:55:55 +0000 2014   \n",
       "4  623495537         1  Tue Dec 02 00:06:05 +0000 2014   \n",
       "\n",
       "                                                text  Unnamed: 4  Unnamed: 5  \n",
       "0  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...         NaN         NaN  \n",
       "1  @apple Contact sync between Yosemite and iOS8 ...         NaN         NaN  \n",
       "2  WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...         NaN         NaN  \n",
       "3  @Apple, For the love of GAWD, CENTER the '1'on...         NaN         NaN  \n",
       "4  i get the storage almost full notification lit...         NaN         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset = pd.read_csv(\"Q2 Sentiment Analysis Dataset.csv\",encoding='latin1')\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3886 entries, 0 to 3885\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          3886 non-null   int64  \n",
      " 1   sentiment   3886 non-null   object \n",
      " 2   date        3886 non-null   object \n",
      " 3   text        3886 non-null   object \n",
      " 4   Unnamed: 4  0 non-null      float64\n",
      " 5   Unnamed: 5  0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 182.3+ KB\n"
     ]
    }
   ],
   "source": [
    "Dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@apple Contact sync between Yosemite and iOS8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@Apple, For the love of GAWD, CENTER the '1'on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i get the storage almost full notification lit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0         1  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...\n",
       "1         1  @apple Contact sync between Yosemite and iOS8 ...\n",
       "2         1  WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...\n",
       "3         1  @Apple, For the love of GAWD, CENTER the '1'on...\n",
       "4         1  i get the storage almost full notification lit..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Dataset.drop(columns=['id','date','Unnamed: 4', 'Unnamed: 5'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the Occurances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3               2162\n",
       "1               1219\n",
       "5                423\n",
       "not_relevant      82\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral         2162\n",
       "negative        1219\n",
       "positive         423\n",
       "not_relevant      82\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = df['sentiment'].replace({'1': 'negative', '3': 'neutral', '5': 'positive'})\n",
    "df.sentiment.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@apple Contact sync between Yosemite and iOS8 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Apple, For the love of GAWD, CENTER the '1'on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>i get the storage almost full notification lit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0  negative  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...\n",
       "1  negative  @apple Contact sync between Yosemite and iOS8 ...\n",
       "2  negative  WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...\n",
       "3  negative  @Apple, For the love of GAWD, CENTER the '1'on...\n",
       "4  negative  i get the storage almost full notification lit..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.str.replace(r'[\\(\\[].*?[\\)\\]]', '')# Remove text in square brackets and parentheses\n",
    "    \n",
    "    text = text.str.replace('\\n', ' ')# Remove newline characters\n",
    "    \n",
    "    text = text.str.lower()# Convert text to lowercase\n",
    "    \n",
    "    text = text.str.replace(r'\\d+', '')# Remove numbers\n",
    "    \n",
    "    text = text.str.replace(r'[@#]', '')# Remove '@' and '#' characters\n",
    "   \n",
    "    text = text.str.replace('[{}]'.format(string.punctuation), '') # Remove punctuation\n",
    "    \n",
    "   \n",
    "    clean_text = []\n",
    "    ignore = set(stopwords.words('english'))  # Remove stopwords from text\n",
    "    \n",
    "    for i in text:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        words = [word for word in words if word not in ignore and len(word) > 1]\n",
    "        res_text = \" \".join(words)\n",
    "        clean_text.append(res_text)\n",
    "        \n",
    "    return clean_text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "C:\\Users\\Hamza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "C:\\Users\\Hamza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Hamza\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_text</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_separated</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...</td>\n",
       "      <td>wtf battery one second ago wtf apple</td>\n",
       "      <td>wtf battery one second ago wtf apple</td>\n",
       "      <td>wtf battery one second ago wtf apple</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@apple Contact sync between Yosemite and iOS8 ...</td>\n",
       "      <td>apple contact sync yosemite ios seriously scre...</td>\n",
       "      <td>apple contact sync yosemite io seriously screw...</td>\n",
       "      <td>apple contact sync yosemite io seriously screw...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...</td>\n",
       "      <td>warning buy iphone unlocked apple iphone use v...</td>\n",
       "      <td>warning buy iphone unlocked apple iphone use v...</td>\n",
       "      <td>warning buy iphone unlocked apple iphone use v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Apple, For the love of GAWD, CENTER the '1'on...</td>\n",
       "      <td>apple love gawd center damn calendar app fixed...</td>\n",
       "      <td>apple love gawd center damn calendar app fixed...</td>\n",
       "      <td>apple love gawd center damn calendar app fixed...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i get the storage almost full notification lit...</td>\n",
       "      <td>get storage almost full notification literally...</td>\n",
       "      <td>get storage almost full notification literally...</td>\n",
       "      <td>get storage almost full notification literally...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            old_text  \\\n",
       "0  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...   \n",
       "1  @apple Contact sync between Yosemite and iOS8 ...   \n",
       "2  WARNING IF YOU BUY AN IPHONE 5S UNLOCKED FROM ...   \n",
       "3  @Apple, For the love of GAWD, CENTER the '1'on...   \n",
       "4  i get the storage almost full notification lit...   \n",
       "\n",
       "                                                text  \\\n",
       "0               wtf battery one second ago wtf apple   \n",
       "1  apple contact sync yosemite ios seriously scre...   \n",
       "2  warning buy iphone unlocked apple iphone use v...   \n",
       "3  apple love gawd center damn calendar app fixed...   \n",
       "4  get storage almost full notification literally...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0               wtf battery one second ago wtf apple   \n",
       "1  apple contact sync yosemite io seriously screw...   \n",
       "2  warning buy iphone unlocked apple iphone use v...   \n",
       "3  apple love gawd center damn calendar app fixed...   \n",
       "4  get storage almost full notification literally...   \n",
       "\n",
       "                                      text_separated sentiment  \n",
       "0               wtf battery one second ago wtf apple  negative  \n",
       "1  apple contact sync yosemite io seriously screw...  negative  \n",
       "2  warning buy iphone unlocked apple iphone use v...  negative  \n",
       "3  apple love gawd center damn calendar app fixed...  negative  \n",
       "4  get storage almost full notification literally...  negative  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df=pd.DataFrame(columns=['old_text','text'])\n",
    "processed_df['old_text'] = df['text']\n",
    "processed_df['text'] = preprocess_text(df['text'])\n",
    "processed_df['text_lemmatized'] = processed_df['text'].apply(lemmatize_text)\n",
    "processed_df['text_separated'] = processed_df['text_lemmatized'].apply(str)\n",
    "processed_df['sentiment'] = df['sentiment']\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3886 entries, 0 to 3885\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   old_text         3886 non-null   object\n",
      " 1   text             3886 non-null   object\n",
      " 2   text_lemmatized  3886 non-null   object\n",
      " 3   text_separated   3886 non-null   object\n",
      " 4   sentiment        3886 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 151.9+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wtf battery one second ago wtf apple</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple contact sync yosemite io seriously screw...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>warning buy iphone unlocked apple iphone use v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple love gawd center damn calendar app fixed...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get storage almost full notification literally...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                word Sentiment\n",
       "0               wtf battery one second ago wtf apple  negative\n",
       "1  apple contact sync yosemite io seriously screw...  negative\n",
       "2  warning buy iphone unlocked apple iphone use v...  negative\n",
       "3  apple love gawd center damn calendar app fixed...  negative\n",
       "4  get storage almost full notification literally...  negative"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df = pd.DataFrame(columns=['word','Sentiment'])\n",
    "analysis_df.word = processed_df.text_separated.apply(str)\n",
    "analysis_df.Sentiment = processed_df.sentiment\n",
    "analysis_df.head()\n",
    "###i have an error here i.e processed_df has 3886 rows so i have to map sentiments to each word bcecause in this case i am getting Nan for words\n",
    "### which are after 3886 rows in word_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_series=pd.DataFrame()\n",
    "label_encoder = LabelEncoder()\n",
    "word_series['Word'] = analysis_df.word\n",
    "word_series['sentiment'] = label_encoder.fit_transform(analysis_df['Sentiment'])\n",
    "sentiments = word_series['sentiment'].unique()\n",
    "\n",
    "word_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_series.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3108,) (3108,)\n",
      "Testing set shape: (778,) (778,)\n"
     ]
    }
   ],
   "source": [
    "# split data \n",
    "X=analysis_df.word\n",
    "y = word_series.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Techniques:\n",
    "<ol>\n",
    "<li>Bag of words based on raw counts </li>\n",
    "<li>Bag of words based on TfIDF </li>\n",
    "<li>ngrams (unigrams, bigrams, trigrams)</li> </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bag of words based on raw counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count_vectorizer = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count_vectorizer = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bag of words based on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = (1, 3)\n",
    "count_vectorizer = CountVectorizer( ngram_range=ngram_range)\n",
    "\n",
    "X_train_ngram =count_vectorizer.fit_transform(X_train)\n",
    "X_test_ngram = count_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logisticRegression(X_train, X_test, y_train, y_test):\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    predictions = lr.predict(X_test)\n",
    "    return classification_report(predictions, y_test)\n",
    "    \n",
    "\n",
    "def get_naiveBayes(X_train, X_test, y_train, y_test):\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    predictions = nb.predict(X_test)\n",
    "    return (classification_report(predictions, y_test))\n",
    "    \n",
    "def get_randomForest(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "    return classification_report(predictions, y_test)\n",
    "\n",
    "def get_svm(X_train, X_test, y_train, y_test):\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    predictions = svm.predict(X_test)\n",
    "    return classification_report(predictions, y_test)\n",
    "    \n",
    "\n",
    "def get_perceptron(X_train, X_test, y_train, y_test):\n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    predictions = perceptron.predict(X_test)\n",
    "    return classification_report(predictions, y_test)\n",
    "\n",
    "def classify(method,X_train,X_test,y_train,y_test):\n",
    "    print(\"Logistic Regression for \",method,\"\\n\",get_logisticRegression(X_train, X_test, y_train, y_test))\n",
    "    print(\"Random Forest for \",method,\"\\n\",get_randomForest(X_train, X_test, y_train, y_test))\n",
    "    print(\"Naive Bayes for\", method,\"\\n\",get_naiveBayes(X_train, X_test, y_train, y_test))\n",
    "    print(\"SVM for \",method,\"\\n\",get_svm(X_train, X_test, y_train, y_test))\n",
    "    print(\"Perceptron for \", method,\"\\n\",get_perceptron(X_train, X_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression for  Bag of words \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.74      0.72       226\n",
      "     neutral       0.87      0.73      0.79       511\n",
      "not_relevant       0.05      0.33      0.09         3\n",
      "    positive       0.24      0.58      0.34        38\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.47      0.60      0.48       778\n",
      "weighted avg       0.79      0.72      0.75       778\n",
      "\n",
      "Random Forest for  Bag of words \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.76      0.68       194\n",
      "     neutral       0.92      0.72      0.81       543\n",
      "not_relevant       0.00      0.00      0.00         2\n",
      "    positive       0.27      0.64      0.38        39\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.45      0.53      0.47       778\n",
      "weighted avg       0.81      0.72      0.75       778\n",
      "\n",
      "Naive Bayes for Bag of words \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.63      0.69       284\n",
      "     neutral       0.82      0.74      0.78       470\n",
      "not_relevant       0.00      0.00      0.00         2\n",
      "    positive       0.15      0.64      0.24        22\n",
      "\n",
      "    accuracy                           0.70       778\n",
      "   macro avg       0.43      0.50      0.43       778\n",
      "weighted avg       0.78      0.70      0.73       778\n",
      "\n",
      "SVM for  Bag of words \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.78      0.70       193\n",
      "     neutral       0.94      0.71      0.81       570\n",
      "not_relevant       0.00      0.00      0.00         0\n",
      "    positive       0.13      0.80      0.22        15\n",
      "\n",
      "    accuracy                           0.73       778\n",
      "   macro avg       0.43      0.57      0.43       778\n",
      "weighted avg       0.85      0.73      0.77       778\n",
      "\n",
      "Perceptron for  Bag of words \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.68      0.70       253\n",
      "     neutral       0.80      0.75      0.77       456\n",
      "not_relevant       0.15      0.43      0.22         7\n",
      "    positive       0.28      0.42      0.34        62\n",
      "\n",
      "    accuracy                           0.70       778\n",
      "   macro avg       0.49      0.57      0.51       778\n",
      "weighted avg       0.73      0.70      0.71       778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify(\"Bag of words\",X_train_count_vectorizer,X_test_count_vectorizer,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression for  Bag of words with TFIDF \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.71      0.70       228\n",
      "     neutral       0.89      0.71      0.79       538\n",
      "not_relevant       0.00      0.00      0.00         0\n",
      "    positive       0.10      0.75      0.17        12\n",
      "\n",
      "    accuracy                           0.71       778\n",
      "   macro avg       0.42      0.54      0.41       778\n",
      "weighted avg       0.82      0.71      0.75       778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest for  Bag of words with TFIDF \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.75      0.68       197\n",
      "     neutral       0.91      0.71      0.80       549\n",
      "not_relevant       0.00      0.00      0.00         0\n",
      "    positive       0.24      0.69      0.35        32\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.44      0.54      0.46       778\n",
      "weighted avg       0.81      0.72      0.75       778\n",
      "\n",
      "Naive Bayes for Bag of words with TFIDF \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.69      0.68       225\n",
      "     neutral       0.90      0.70      0.79       550\n",
      "not_relevant       0.00      0.00      0.00         0\n",
      "    positive       0.03      1.00      0.06         3\n",
      "\n",
      "    accuracy                           0.70       778\n",
      "   macro avg       0.40      0.60      0.38       778\n",
      "weighted avg       0.82      0.70      0.75       778\n",
      "\n",
      "SVM for  Bag of words with TFIDF \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.75      0.70       210\n",
      "     neutral       0.92      0.71      0.80       551\n",
      "not_relevant       0.00      0.00      0.00         0\n",
      "    positive       0.16      0.88      0.27        17\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.43      0.59      0.44       778\n",
      "weighted avg       0.83      0.72      0.76       778\n",
      "\n",
      "Perceptron for  Bag of words with TFIDF \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.70      0.69       232\n",
      "     neutral       0.81      0.73      0.77       471\n",
      "not_relevant       0.15      0.43      0.22         7\n",
      "    positive       0.33      0.46      0.39        68\n",
      "\n",
      "    accuracy                           0.70       778\n",
      "   macro avg       0.49      0.58      0.52       778\n",
      "weighted avg       0.72      0.70      0.71       778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classify(\"Bag of words with TFIDF\",X_train_tfidf,X_test_tfidf,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression for  Ngrams \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.73      0.70       218\n",
      "     neutral       0.89      0.71      0.79       532\n",
      "not_relevant       0.00      0.00      0.00         3\n",
      "    positive       0.19      0.72      0.31        25\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.44      0.54      0.45       778\n",
      "weighted avg       0.80      0.72      0.75       778\n",
      "\n",
      "Random Forest for  Ngrams \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.81      0.68       171\n",
      "     neutral       0.94      0.70      0.80       579\n",
      "not_relevant       0.00      0.00      0.00         1\n",
      "    positive       0.23      0.78      0.35        27\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.44      0.57      0.46       778\n",
      "weighted avg       0.84      0.72      0.76       778\n",
      "\n",
      "Naive Bayes for Ngrams \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.64      0.69       277\n",
      "     neutral       0.81      0.74      0.78       470\n",
      "not_relevant       0.05      0.25      0.08         4\n",
      "    positive       0.18      0.63      0.28        27\n",
      "\n",
      "    accuracy                           0.70       778\n",
      "   macro avg       0.45      0.57      0.46       778\n",
      "weighted avg       0.77      0.70      0.73       778\n",
      "\n",
      "SVM for  Ngrams \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.79      0.69       184\n",
      "     neutral       0.94      0.69      0.80       584\n",
      "not_relevant       0.00      0.00      0.00         0\n",
      "    positive       0.09      0.80      0.16        10\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.41      0.57      0.41       778\n",
      "weighted avg       0.85      0.72      0.76       778\n",
      "\n",
      "Perceptron for  Ngrams \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.74      0.72       224\n",
      "     neutral       0.87      0.73      0.79       511\n",
      "not_relevant       0.05      0.25      0.08         4\n",
      "    positive       0.24      0.56      0.33        39\n",
      "\n",
      "    accuracy                           0.72       778\n",
      "   macro avg       0.46      0.57      0.48       778\n",
      "weighted avg       0.78      0.72      0.75       778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classify(\"Ngrams\",X_train_ngram,X_test_ngram,y_train,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

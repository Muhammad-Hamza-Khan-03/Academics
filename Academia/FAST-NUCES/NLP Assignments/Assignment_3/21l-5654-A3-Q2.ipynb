{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I saved pre processed text in csv file m using that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ایٹم بم بنایا ھے بھائی ایٹم بمب کوٹ لکھپت اتفا...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چندے انقلاب عمران خان وزیر اعظم بن سکتے</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ٹویٹر خیال</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سرچ انجن گوگل نائب صدر فضا فٹ بلندی چھلانگ عال...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اسکی لہریں یار أ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Class\n",
       "0  ایٹم بم بنایا ھے بھائی ایٹم بمب کوٹ لکھپت اتفا...      2\n",
       "1            چندے انقلاب عمران خان وزیر اعظم بن سکتے      0\n",
       "2                                         ٹویٹر خیال      1\n",
       "3  سرچ انجن گوگل نائب صدر فضا فٹ بلندی چھلانگ عال...      2\n",
       "4                                   اسکی لہریں یار أ      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urdu_dataset = pd.read_csv(\"Analysed_Urdu_Tweets.csv\",index_col=False)\n",
    "urdu_dataset.drop(urdu_dataset.columns[0],axis=1,inplace=True)\n",
    "urdu_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (749, 31)\n",
      "Testing set shape: (250, 31)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## WORDS in dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(urdu_dataset['Tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(urdu_dataset['Tweet'])\n",
    "\n",
    "max_sequence_length = max(len(x) for x in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "## Split the dataset\n",
    "# Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, urdu_dataset['Class'], test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\",X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25768\\809091874.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword2vec_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word2vec-google-news-300\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mglove_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove-twitter-200\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# glove_vectors = api.load(\"glove-wiki-gigaword-100\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1723\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1725\u001b[1;33m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1726\u001b[0m         )\n\u001b[0;32m   1727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2064\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m         \u001b[0mkv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python64\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# formerly known as syn0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import tensorflow_hub as hub\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "glove_vectors = api.load(\"glove-twitter-200\")\n",
    "# glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(seq) for seq in X_train])\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_weights = create_embedding_matrix(word2vec_model, tokenizer)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_matrix = np.zeros((vocab_size,word2vec_model.vector_size))\n",
    "\n",
    "for w,i in tokenizer.word_index.items():\n",
    "    if w in word2vec_model:\n",
    "        embed_matrix[i] = word2vec_model[w]\n",
    "word2vec_weights = embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM -> 3,0.7 ->0.66\n",
    "# nested for for dropout and num_layers\n",
    "\n",
    "num_layers =3\n",
    "dropout_rate=0.7\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "18/18 [==============================] - 13s 174ms/step - loss: 0.0000e+00 - accuracy: 0.4082 - val_loss: 0.0000e+00 - val_accuracy: 0.5160\n",
      "Epoch 2/8\n",
      "18/18 [==============================] - 1s 70ms/step - loss: 0.0000e+00 - accuracy: 0.4545 - val_loss: 0.0000e+00 - val_accuracy: 0.5160\n",
      "Epoch 3/8\n",
      "18/18 [==============================] - 1s 72ms/step - loss: 0.0000e+00 - accuracy: 0.4100 - val_loss: 0.0000e+00 - val_accuracy: 0.5160\n",
      "Epoch 4/8\n",
      "18/18 [==============================] - 1s 74ms/step - loss: 0.0000e+00 - accuracy: 0.4545 - val_loss: 0.0000e+00 - val_accuracy: 0.2234\n",
      "Epoch 5/8\n",
      "18/18 [==============================] - 2s 84ms/step - loss: 0.0000e+00 - accuracy: 0.4670 - val_loss: 0.0000e+00 - val_accuracy: 0.1968\n",
      "Epoch 6/8\n",
      "18/18 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - accuracy: 0.4920 - val_loss: 0.0000e+00 - val_accuracy: 0.2819\n",
      "Epoch 7/8\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.0000e+00 - accuracy: 0.4955 - val_loss: 0.0000e+00 - val_accuracy: 0.2766\n",
      "Epoch 8/8\n",
      "18/18 [==============================] - 1s 75ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.2766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b72f4c9788>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=8, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = dict()\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 18ms/step\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.2880\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "model_eval['LSTM(no embedding)'] = {'Precision':precision,'Recall':recall,'F1 score':f1,'accuracy':accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LSTM(no embedding)': {'Precision': 0.21623376623376625,\n",
       "  'Recall': 0.3551912568306011,\n",
       "  'F1 score': 0.2119744922059518,\n",
       "  'accuracy': 0.2879999876022339}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_weights), output_dim=300,weights=[word2vec_weights], input_length=max_length))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 11s 381ms/step - loss: 0.0000e+00 - accuracy: 0.2371 - val_loss: 0.0000e+00 - val_accuracy: 0.0067\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.0000e+00 - accuracy: 0.2721 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.0000e+00 - accuracy: 0.3656 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.0000e+00 - accuracy: 0.4808 - val_loss: 0.0000e+00 - val_accuracy: 0.4933\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 2s 155ms/step - loss: 0.0000e+00 - accuracy: 0.4841 - val_loss: 0.0000e+00 - val_accuracy: 0.3200\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.0000e+00 - accuracy: 0.5058 - val_loss: 0.0000e+00 - val_accuracy: 0.3067\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0000e+00 - accuracy: 0.5058 - val_loss: 0.0000e+00 - val_accuracy: 0.2867\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.0000e+00 - accuracy: 0.5058 - val_loss: 0.0000e+00 - val_accuracy: 0.2933\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 2s 144ms/step - loss: 0.0000e+00 - accuracy: 0.5075 - val_loss: 0.0000e+00 - val_accuracy: 0.3133\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.0000e+00 - accuracy: 0.5092 - val_loss: 0.0000e+00 - val_accuracy: 0.3867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b72f94e248>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 22ms/step\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.3800\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "precision2 = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall2 = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f12 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "loss, accuracy2 = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval['LSTM(embedding: word2vec)'] = {'Precision':precision2,'Recall':recall2,'F1 score':f12,'accuracy':accuracy2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Precision    Recall  F1 score  accuracy\n",
      "LSTM(no embedding)          0.216234  0.355191  0.211974     0.288\n",
      "LSTM(embedding: word2vec)   0.190573  0.312386  0.223668     0.380\n"
     ]
    }
   ],
   "source": [
    "table = pd.DataFrame.from_dict(model_eval, orient='index')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_vectors.key_to_index:\n",
    "        embedding_matrix[i] = glove_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=3, activation='softmax')) #3 classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 7s 148ms/step - loss: 1.0097 - accuracy: 0.4980 - val_loss: 0.8262 - val_accuracy: 0.4880\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.7971 - accuracy: 0.4900 - val_loss: 0.7908 - val_accuracy: 0.4880\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.7820 - accuracy: 0.4806 - val_loss: 0.7891 - val_accuracy: 0.4880\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.7754 - accuracy: 0.5033 - val_loss: 0.7937 - val_accuracy: 0.4880\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.7798 - accuracy: 0.5060 - val_loss: 0.8074 - val_accuracy: 0.4880\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.7901 - accuracy: 0.4806 - val_loss: 0.7892 - val_accuracy: 0.4880\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.7731 - accuracy: 0.5007 - val_loss: 0.7945 - val_accuracy: 0.4880\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7721 - accuracy: 0.5007 - val_loss: 0.7905 - val_accuracy: 0.4880\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7732 - accuracy: 0.4953 - val_loss: 0.7920 - val_accuracy: 0.4880\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.7708 - accuracy: 0.5033 - val_loss: 0.7918 - val_accuracy: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b74a9fba08>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7088 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 27ms/step - loss: 0.7918 - accuracy: 0.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy3 = model.evaluate(X_test, y_test)\n",
    "precision3 = precision_score(y_test, y_pred_labels, average='macro')\n",
    "recall3 = recall_score(y_test, y_pred_labels, average='macro')\n",
    "f13 = f1_score(y_test, y_pred_labels, average='macro')\n",
    "model_eval['LSTM(embedding: GLOVE)'] = {'Precision':precision3,'Recall':recall3,'F1 score':f13,'accuracy':accuracy3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((vocab_size,fasttext_model.vector_size))\n",
    "\n",
    "for w,i in tokenizer.word_index.items():\n",
    "    if w in fasttext_model and w in word2vec_model:\n",
    "        embed_matrix[i] = word2vec_model[w]\n",
    "fasttext_weights = embed_matrix    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25140\\4205886198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_dim' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim = len(fasttext_weights)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, weights=[embedding_matrix], input_length=input_length, trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "loss, accuracy4 = model.evaluate(X_test, y_test)\n",
    "model_eval['LSTM(embedding: fast-text)'] = {'Precision':precision,'Recall':recall,'F1 score':f1,'accuracy':accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Precision    Recall  F1 score  accuracy\n",
      "LSTM(no embedding)         0.162667  0.333333  0.218638       NaN\n",
      "GRU(embedding: word2vec)   0.162667  0.333333  0.218638       NaN\n",
      "LSTM(embedding: GLOVE)     0.162667  0.333333  0.218638     0.488\n"
     ]
    }
   ],
   "source": [
    "table = pd.DataFrame.from_dict(model_eval, orient='index')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
